# -----------------------------------------------------------------------------------------
# EXPERIMENT CONFIGURATIONS
# -----------------------------------------------------------------------------------------
experiment_name: GAGEii - Missing Value Test  # A user-defined name for this experiment/run

run_dir:  # (Optional) directory to store experiment output. If empty, defaults to ./runs/<experiment_name>/

train_basin_file: basin_list/missing_involved.txt    # File listing basins for training
validation_basin_file: basin_list/missing_involved.txt # File listing basins for validation
test_basin_file: basin_list/missing_involved.txt     # File listing basins for testing

train_start_date: '01/01/1981'  # Start date for training data
train_end_date: '31/12/2000'    # End date for training data
validation_start_date: '01/01/2001'   # Start date for validation data
validation_end_date: '31/12/2010'     # End date for validation data
test_start_date: '01/01/2011'         # Start date for test data
test_end_date: '31/12/2022'           # End date for test data

seed:  # (Optional) fixed random seed for reproducibility. Leave empty for a random seed

device: cuda:0  # Which device to use for training ('cuda:X', 'cpu', etc.)

# -----------------------------------------------------------------------------------------
# VALIDATION CONFIGURATION
# -----------------------------------------------------------------------------------------
validate_every: 1              # Perform validation after every N epochs
validate_n_random_basins: 200  # Number of basins randomly chosen for validation each time
metrics:
  - NSE   # List of metrics to calculate (here, only Nash-Sutcliffe Efficiency)

# -----------------------------------------------------------------------------------------
# MODEL CONFIGURATION
# -----------------------------------------------------------------------------------------
model: cudalstm           # Base model type from the NeuralHydrology model zoo
head: regression          # Prediction head (options include 'regression', 'mdn', 'umal', etc.)
hidden_size: 256          # LSTM hidden state size
initial_forget_bias: 3    # Initial bias for the forget gate in LSTM cells
output_dropout: 0.2       # Dropout rate for the LSTM output
output_activation: linear # Activation function after the final output layer

# -----------------------------------------------------------------------------------------
# TRAINING CONFIGURATION
# -----------------------------------------------------------------------------------------
optimizer: Adam       # Which optimizer to use (Adam, Adadelta, etc.)
loss: RMSE            # Loss function (MSE, NSE, RMSE, etc.)

learning_rate:        # Dictionary specifying learning rates at given epochs
  0: 0.001            # 0 -> epoch 0: initial LR = 0.001
  50: 0.0005          # 50 -> epoch 50: LR = 0.0005

batch_size: 256       # Number of samples per training mini-batch
epochs: 50            # Total number of training epochs
clip_gradient_norm: 1 # If non-zero, clips gradients to this norm for stability
predict_last_n: 1     # Number of time steps used to compute the loss (1 = only last time step in sequence)
seq_length: 365       # Length of the input sequence (in days here) fed to the model
num_workers: 8        # Number of parallel workers for the data loading pipeline
log_interval: 5       # Log the training loss every N steps
log_tensorboard: True # Whether to log training metrics to TensorBoard
save_weights_every: 1 # Save model weights to disk every N epochs
save_validation_results: True # If True, store validation results to disk after each validation

# -----------------------------------------------------------------------------------------
# DATA CONFIGURATIONS
# -----------------------------------------------------------------------------------------
data_dir: C:/Users/ybrot/Desktop/course/UROP/GAGEii_modeling/data
  # Directory where your CSV or other data files are located
dataset: generic
  # Specifies the data source/format. 'generic' means your data is in a generic CSV format
  # recognized by NeuralHydrology.

# Which columns (time-series variables) to use as model inputs
dynamic_inputs:
  - dayl   
  - prcp   
  - srad   
  - swe    
  - tmax   
  - tmin   
  - vp     

# Which columns to use as the target for training
target_variables:
  - discharge  # Streamflow/discharge as the prediction target

# -----------------------------------------------------------------------------------------
# PREPROCESSING CONFIGURATIONS
# -----------------------------------------------------------------------------------------
preprocessing:
  # Global options: these apply to all variables unless overridden for a specific variable.
  fill_nans: None      # How to fill missing values: 'mean', 'median', 'zeros', None, etc

  # Definitions for how each dynamic input variable is transformed
  dynamic:
    dayl:
      scaler: "standard"     # 'standard' = subtract mean, divide by standard deviation
      log_transform: False   # No log transform for this variable
    prcp:
      scaler: "minmax"       # 'minmax' = scale values into [0,1]
      log_transform: False
    srad:
      scaler: "robust"       # 'robust' = uses median & interquartile range (less sensitive to outliers)
      log_transform: False
    swe:
      scaler: "standard"
      log_transform: True    # Apply log(x + 1) transform (commonly adds 1 to avoid log(0))
    tmax:
      scaler: "minmax"
      log_transform: False
    tmin:
      scaler: "minmax"
      log_transform: False
    vp:
      scaler: "standard"
      log_transform: False

  # Definitions for how each target variable is transformed
  target:
    discharge:
      scaler: "standard"     # 'standard' scaling for discharge
      log_transform: True     # Log-transform for skewed discharge distribution
