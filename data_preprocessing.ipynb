{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Interploate the missing dates.\n",
    "\n",
    "The following dates are missing in the dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates:\n",
      "1980-12-31\n",
      "1984-12-31\n",
      "1988-12-31\n",
      "1992-12-31\n",
      "1996-12-31\n",
      "2000-12-31\n",
      "2004-12-31\n",
      "2008-12-31\n",
      "2012-12-31\n",
      "2016-12-31\n",
      "2020-12-31\n",
      "Detected frequency: None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "basin_id = \"05535070\"\n",
    "file_path = f'c:/Users/ybrot/Desktop/course/UROP/GAGEii_modeling/US_climate_discharge_data/{basin_id}.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Generate a date range from the minimum to the maximum date in the dataframe\n",
    "date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "\n",
    "# Reindex the dataframe to this date range\n",
    "df_reindexed = df.reindex(date_range)\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = df_reindexed[df_reindexed.isnull().any(axis=1)].index\n",
    "\n",
    "# Print missing dates\n",
    "print(\"Missing dates:\")\n",
    "for date in missing_dates:\n",
    "    print(date.strftime('%Y-%m-%d'))\n",
    "\n",
    "# Check frequency\n",
    "frequency = pd.infer_freq(df.index)\n",
    "print(f\"Detected frequency: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will interpolate the missing dates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [03:33<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation and file writing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = 'climate_discharge_data'\n",
    "output_folder = 'data/csv_files'\n",
    "basins_list_file = 'basin_list/basin_list_complete.txt'\n",
    "\n",
    "# Read the list of basin IDs\n",
    "with open(basins_list_file, 'r') as file:\n",
    "    basin_ids = file.read().splitlines()\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to interpolate missing dates\n",
    "def interpolate_missing_dates(df, missing_dates):\n",
    "    # Ensure the 'date' column is in datetime format\n",
    "    df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Set the 'date' column as the index\n",
    "    df.set_index('date', inplace=True)\n",
    "    # Generate a complete date range from the minimum to the maximum date in the dataframe\n",
    "    date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    # Reindex the dataframe to this date range\n",
    "    df_reindexed = df.reindex(date_range)\n",
    "    # Interpolate missing values\n",
    "    df_interpolated = df_reindexed.interpolate(method='time')\n",
    "    return df_interpolated\n",
    "\n",
    "# Process each CSV file\n",
    "for basin_id in tqdm(basin_ids):\n",
    "    input_file = os.path.join(input_folder, f'{basin_id}.csv')\n",
    "    output_file = os.path.join(output_folder, f'{basin_id}.csv')\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Interpolate missing dates\n",
    "    missing_dates = [\n",
    "        '1980-12-31', '1984-12-31', '1988-12-31', '1992-12-31', \n",
    "        '1996-12-31', '2000-12-31', '2004-12-31', '2008-12-31', \n",
    "        '2012-12-31', '2016-12-31', '2020-12-31'\n",
    "    ]\n",
    "    df_interpolated = interpolate_missing_dates(df, missing_dates)\n",
    "    \n",
    "    # Reset the index to have 'date' as a column again\n",
    "    df_interpolated.reset_index(inplace=True)\n",
    "    df_interpolated.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    # Convert all negative values in the \"discharge\" column to NaN\n",
    "    df_interpolated['discharge'] = df_interpolated['discharge'].apply(lambda x: x if x >= 0 else pd.NA)\n",
    "    \n",
    "    # If the basin_id does not start with \"02\", divide the values in the \"discharge\" column by 35.315\n",
    "    # Basins not starting with \"02\" are US stations, using an undesired unit - cubic ft / s\n",
    "    if not basin_id.startswith(\"02\"):\n",
    "        df_interpolated['discharge'] = df_interpolated['discharge'] / 35.315\n",
    "    \n",
    "    # Write the output CSV file\n",
    "    df_interpolated.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Interpolation and file writing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the data for missing dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:16<00:00, 60.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files meet the condition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the imputed CSV files\n",
    "directory = 'data/csv_files'\n",
    "\n",
    "# Initialize a flag to check if all files meet the condition\n",
    "all_files_valid = True\n",
    "non_satisfying_files = []\n",
    "\n",
    "# Get the list of all file names in the directory\n",
    "basin_ids = [f.split('.')[0] for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for basin_id in tqdm(basin_ids):\n",
    "    \n",
    "    file_path = f'data/csv_files/{basin_id}.csv'\n",
    "    file_name = f'{basin_id}.csv'\n",
    "        \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "    # Ensure the 'date' column is in datetime format and set as index\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Create a full date range and check for missing dates\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    missing_dates = full_range.difference(df.index)\n",
    "    \n",
    "    # Check if the frequency of missing dates is 'D'\n",
    "    if missing_dates.freq != 'D':\n",
    "        # print(f\"File {file_name} does not meet the condition.\")\n",
    "        # print(f\"Frequency of missing dates: {missing_dates.freq}\")\n",
    "        all_files_valid = False\n",
    "        non_satisfying_files.append(file_name)\n",
    "        \n",
    "\n",
    "if all_files_valid:\n",
    "    print(\"All files meet the condition.\")\n",
    "else:\n",
    "    print(\"Some files do not meet the condition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the data to a time series. (NetCDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_conversion import DataFrame_to_CDF\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def convert_csv_to_cdf(input_folder: str, output_folder: str):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    for csv_file in tqdm(csv_files):\n",
    "        # Construct the file path for the CSV file\n",
    "        csv_file_path = os.path.join(input_folder, csv_file)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file_path, index_col='date')\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # Convert the DataFrame to a CDF file\n",
    "        basin_id = os.path.splitext(csv_file)[0]\n",
    "        DataFrame_to_CDF(df, output_folder, basin_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [01:31<00:00, 10.65it/s]\n"
     ]
    }
   ],
   "source": [
    "input_folder = './data/csv_files'\n",
    "output_folder = './data/time_series'\n",
    "convert_csv_to_cdf(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:37<00:00, 26.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "data_folder = 'data/csv_files'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data_list = []\n",
    "\n",
    "# Iterate over all CSV files in the directory\n",
    "for file_name in tqdm(os.listdir(data_folder)):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        df = pd.read_csv(file_path, index_col='date')\n",
    "        data_list.append(df)\n",
    "\n",
    "# Concatenate all data into a single DataFrame\n",
    "combined_df = pd.concat(data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of each variable\n",
    "variables = ['discharge', 'dayl', 'prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp']\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, variable in tqdm(enumerate(variables, 1)):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(combined_df[variable].dropna(), bins=30, kde=True)\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {variable}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
